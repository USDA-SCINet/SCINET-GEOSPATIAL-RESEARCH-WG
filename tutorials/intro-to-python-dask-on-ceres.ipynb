{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing on Ceres with Python and Dask\n",
    "(adapted from https://github.com/willirath/dask_jobqueue_workshop_materials)\n",
    "\n",
    "## The Goal\n",
    "\n",
    "Interactive data analysis on very large datasets. The tools in this tutorial are most appropriate for analysis of large earth-science-type datasets. \n",
    "\n",
    "For large dataset analysis, you'll want to run parallel (instead of serial) computations to save time. On a high-performance computer (HPC), you could divide your computing into independent segments (batching) and submit multiple batch scripts to run compute jobs simultaneously or you could parallelize your codes using MPI (Message Passing Interface), a traditional method for parallel computing if your code is in C or Fortran. Actually, there is also an \"MPI for Python\" package, but the methods in this tutorial are much *much* simpler. Both the batching and MPI methods of parallelization do not allow for interactive analysis, such as analysis using a Jupyter notebook, which is often desire by the earth science research community. Note that interactive analysis here does *not* mean constant visual presentation of the data with a graphical user interface (GUI) such as in ArcGIS. \n",
    "\n",
    "For earth-science-type data and analysis with Python, one of the simplest ways to run parallel computations in an interactive environment is with the Dask package.\n",
    "\n",
    "\n",
    "## Core Lessons\n",
    "\n",
    "Using Dask to:\n",
    "- set up SLURM clusters\n",
    "- scale clusters\n",
    "- use adaptive clusters\n",
    "- view Dask diagnostics\n",
    "\n",
    "This tutorial will demonstrate how to use Dask to manage compute jobs on a SLURM cluster (including setting up your SLURM compute cluster, scaling the cluster, and how to use an adaptive cluster to save compute resources for others). The tutorial will also explain how to access the Dask diagnostics dashboard to view the cluster working in real time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "THIS SECTION WILL BE PULLED FROM THE NOTEBOOK AND POSTED IN A TEXT FILE ALONGSIDE IT SO THAT PEOPLE WHO AREN'T YET SET UP ON THE HPC CAN ACCESS THE SETUP INSTRUCTIONS\n",
    "    \n",
    "## Pre-Tutorial Setup\n",
    "#### 1. Be able to login to the Ceres HPC \n",
    "\n",
    "   You should receive email instructions for logging in and setting up multifactor authentication when your SCINet account is approved. \n",
    "   Instructions can also be found in the [Ceres User Guide](https://scinet.usda.gov/guide/ceres/#logging-in-to-scinet) and the [Multifactor Authentication Guide](https://scinet.usda.gov/guide/multifactor/#authentication-on-your-computer-using-authy). \n",
    "   Contact the VRSC at scinet_vrsc@usda.gov if you have problems.\n",
    "    \n",
    "#### 2. Get a Github account \n",
    "    \n",
    "   If you don't already have a Github account, sign up for a free account at https://github.com/join.\n",
    "    \n",
    "#### 3. Software setup\n",
    "   \n",
    "   a. Install miniconda on your Ceres account. Experienced HPC users can go to https://docs.conda.io/en/latest/miniconda.html and install the latest version of miniconda either in your home directory or your project /KEEP directory, and then keep conda up to date using \"conda update conda\". \n",
    "   \n",
    "   Another option, which may be better for newer HPC users, is to load the miniconda module that is already installed on Ceres. The only downside is that it may be slightly outdated and you will have to load the module every time you want to use it. Follow the instructions in the [User-Installed Software of Ceres with Conda Guide](https://scinet.usda.gov/guide/conda/).\n",
    "    \n",
    "#### 4. Clone the tutorials repository from Github to your Ceres account\n",
    "   \n",
    "   go to the repository at xx\n",
    "   click \"Clone or Download\" and copy the http link to your clipboard\n",
    "   login to the Ceres HPC and navigate to where you want to copy the repository \n",
    "   issue the command: git clone paste-the-repository-link-here\n",
    "   \n",
    "#### 5. Activate the python environment for this tutorial\n",
    "   \n",
    "   Copy the python environment for this tutorial xx located at xx to your conda environments folder, which should be located wherever you installed conda then under xx/xx/xx.\n",
    "   Activate the python environment using \"xx\".\n",
    "\n",
    "#### 6. Open JupyterLab (or JupyterHub?) to execute the tutorial\n",
    "   follow the instructions at xx\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Tutorial: Parallel Computing on Ceres with Python and Dask\n",
    "\n",
    "In this tutorial we will compute in parallel using Python's Dask package to communicate with the Ceres HPC SLURM job scheduler. \n",
    "\n",
    "SLURM (Simple Linux Utility for Resource Management) is a workload manager for HPC systems. From the [SLURM documentation](https://slurm.schedmd.com/quickstart.html), SLURM is \"an open source... cluster management and job scheduling system for large and small Linux clusters. As a cluster workload manager, SLURM has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.\"\n",
    "\n",
    "## Set up a SLURM Cluster with Dask\n",
    "\n",
    "The first step is to create a SLURM cluster using the dask.distributed and dask_jobqueue packages. The SLURMCluster function can be interpreted as the settings/parameters for 1 SLURM compute job. Later, we can increase our compute power by \"scaling our cluster\", which means Dask will ask the SLURM scheduler to execute more than one job at a time for any given computation.\n",
    "<br><br>\n",
    "\n",
    "**Here's a key to the dask_jobqueue.SLURMCluster input parameters in the code block below:**\n",
    "\n",
    "**cores** = Number of logical cores per job. This will be divided among the processes/workers. Can't be more than the lowest number of logical cores per node in the queue you choose, see https://scinet.usda.gov/guide/ceres/#partitions-or-queues.\n",
    "   \n",
    "**processes** = Number of processes per job (also known as Dask \"workers\" or \"worker processes\"). The number of cores per worker will be cores/processes. Can use 1 but more than 1 may help keep your computations running if cores/workers fail. For numeric computations (Numpy, Pandas, xarray, etc.), less workers may run significantly faster due to reduction in communication time. If your computations are mostly pure Python, it may be better to run with many workers each associated with only 1 core. [Here is more info than you'll probably ever want to know about Dask workers](https://distributed.dask.org/en/latest/worker.html). \n",
    "\n",
    "**memory** =  Memory per job. This will be divided among the processes/workers. See https://scinet.usda.gov/guide/ceres/#partitions-or-queues for the maximum memory per core you can request on each queue. **WHY LESS GB WHEN REQUESTING 3GB PER CORE = 120GB ON SHORT? GB vs GiB?** \n",
    "\n",
    "**queue** = Name of the Ceres queue, a.k.a. partition (e.g. short, medium, long, long60, mem, longmem, mem768, debug, brief-low, scavenger, etc.).\n",
    "\n",
    "**walltime** = Time allowed before the job is timed out.\n",
    "\n",
    "**local_directory** = local spill location if the core memory is exceeded **(DO ALL NODES HAVE LOCALLY ATTACHED STORAGE? WHAT TO PUT HERE /SCRATCH, $LOCAL_STORAGE? IF NOT WE HAVE TO MODIFY THE CONFIG FILE TO TURN OF SPILL OVER ~/.config/dask/jobqueue.yaml)**\n",
    "\n",
    "**log_directory** = Location to write the stdout and stderr files for each worker process. Simplest choice may be the directory you are running your code from. \n",
    "<br><br>\n",
    "\n",
    "You can view additional parameters, methods, and attributes in the Dask documentation for [dask_jobqueue.SLURMCluster](https://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SLURMCluster.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import os\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    cores=38,\n",
    "    processes=1,\n",
    "    memory=\"114GB\", #38 cores x 3GB/core\n",
    "    queue=\"short\",\n",
    "    walltime=\"00:10:00\",\n",
    "    log_directory=\"/project/geospatial_user_test_geil/dask_jobqueue_workshop_materials/notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only set up a cluster, we have not started any compute jobs/workers running yet. We can verify this by issuing the following command in a Ceres terminal:\n",
    "```\n",
    "squeue -u firstname.lastname\n",
    "```\n",
    "\n",
    "To see the job script that will be used to start a job running on the Ceres HPC use the method .job_script() as shown in the code block below.\n",
    "<br><br>\n",
    "\n",
    "**Here's a key to the output of the cluster.job_script() command below:**\n",
    "\n",
    "**-J** = Name of the job. This will appear in the \"Name\" column of the squeue output. \"dask-worker\" is the default value.\n",
    "\n",
    "**-e and -o** = Name/Location of the stdout and stderr files for each job. This comes from the SLURMCLuster \"log_directory\" parameter.\n",
    "\n",
    "**-p** = Name of the Ceres queue/partition. This comes from the SLURMCLuster \"queue\" parameter.\n",
    "\n",
    "**-n** = Number of nodes. **ARE WE ALWAYS LIMITED TO 1 NODE FOR THE SETUP JOB? I GOT ERRORS WHEN PUTTING MORE CORES THAN 40. NEED TO TRY USING THE EXTRA PARAMETER TO SPECIFY NUMBER OF NODES**\n",
    "\n",
    "**--cpus-per-task** = Number of cores per job (same as -N). This comes from the SLURMCluster \"cores\" parameter.\n",
    "\n",
    "**--mem** = Memory per job. This comes from the SLURMCluster \"memory\" parameter. \n",
    "\n",
    "**-t** = Time allowed before the job is timed out. This comes from the SLURMCluster parameter \"walltime\".\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Next, we must initialize a Dask Client, which opens a line of communication between Dask worker processes and the SLURM job scheduler by pointing to the address of the scheduler (tcp://10.1.8.84:41601).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.136:45973</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.136:8787/status' target='_blank'>http://10.1.4.136:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.136:45973' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: So far we have only set up a cluster and initialized a client. We still have not started any compute jobs running yet, as shown in the Cluster information above. We can also verify that no workers are running yet by issuing the squeue command in a Ceres terminal again as we did previously or we could access the Dask Diagnostics Dashboard for even more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Dask Diagnostics Dashboard\n",
    "\n",
    "We will now take a look at the Dashboard to verify that there a no workers running in our cluster yet. Once we start computing, we will be able to use the Dashboard to see a visual representation of all the workers running.\n",
    "\n",
    "INSERT INSTRUCTIONS TO OPEN THE DASHBOARD either port forward or better, figure out how to get the Dask extension working in JupyterLab/Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Cluster to Start Computing\n",
    "\n",
    "Now let's start multiple SLURM jobs computing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep   #time for timing how long our computations take, sleep for pausing while SLURM starts the requested jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.136:45973</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.136:8787/status' target='_blank'>http://10.1.4.136:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>38</li>\n",
       "  <li><b>Memory: </b>114.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.136:45973' processes=1 threads=38, memory=114.00 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.scale(jobs=3)  # scale to more jobs\n",
    "sleep(15)              # pause while SLURM starts up the jobs\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .scale() method actually starts the jobs running as shown in the Cluster information above. A quick check of squeue will now show your multiple jobs running as well.\n",
    "\n",
    "When we set up our original cluster (equivalent of 1 SLURM job) we requested 40 cores spread over 2 workers. When we scaled our cluster to 3 jobs we now have 40x3=120 cores spread over 2x3=6 workers, as shown above. Note: you can also scale your cluster by cores, workers or memory as opposed to jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Estimate of $\\pi$\n",
    "\n",
    "Now we will use the [Monte-Carlo method of estimating $\\pi$](https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods)  to demonstrate how Dask can execute parallel computations with the SLURM Cluster we've built and scaled.\n",
    "\n",
    "We estimate the number $\\pi$ by exploiting that the area of a quarter circle of unit radius is $\\pi/4$ and that hence the probability of any randomly chosen point in a unit square to lie in a unit circle centerd at a corner of the unit square is $\\pi/4$ as well. \n",
    "\n",
    "So for N randomly chosen pairs $(x, y)$ with $x\\in[0, 1)$ and $y\\in[0, 1)$, we count the number $N_{circ}$ of pairs that also satisfy $(x^2 + y^2) < 1$ and estimate $\\pi \\approx 4 \\cdot N_{circ} / N$.\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/84/Pi_30K.gif\" \n",
    "     width=\"50%\" \n",
    "     align=top\n",
    "     alt=\"PI monte-carlo estimate\">](https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Let's define a function to compute $\\pi$ and another function to print out some info during the compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "def calc_pi_mc(size_in_bytes, chunksize_in_bytes=200e6):  # IS THERE A REASON TO CHOOSE THIS CHUNKSIZE\n",
    "    \"\"\"Calculate PI using a Monte Carlo estimate.\"\"\"\n",
    "    \n",
    "    size = int(size_in_bytes / 8)      # size= # of random numbers to generate (x & y vals), divide 8 bcz numpy float64 numbers generated by random.uniform use up 8 bytes\n",
    "    chunksize = int(chunksize_in_bytes / 8)\n",
    "    \n",
    "    xy = da.random.uniform(0, 1,                          # this generates a set of x and y value pairs on the interval [0,1) of type float64\n",
    "                           size=(size / 2, 2),            # divide 2 because we are generating an equal number of x and y values (to get our points)\n",
    "                           chunks=(chunksize / 2, 2))     # WHY WOULD WE DIVIDE THE CHUNKSIZE BY 2 HERE????\n",
    "    \n",
    "    in_circle = ((xy ** 2).sum(axis=-1) < 1)     # a boolean array, True for points that fall inside the unit circle (x**2 + y**2 < 1)\n",
    "    pi = 4 * in_circle.mean()                    # mean= sum the number of True elements, divide by the total number of elements in the array\n",
    "\n",
    "    return pi\n",
    "\n",
    "def print_pi_stats(size, pi, time_delta, num_workers):  \n",
    "    \"\"\"Print pi, calculate offset from true value, and print some stats.\"\"\"\n",
    "    print(f\"{size / 1e9} GB\\n\"\n",
    "          f\"\\tMC pi: {pi : 13.11f}\"\n",
    "          f\"\\tErr: {abs(pi - np.pi) : 10.3e}\\n\"\n",
    "          f\"\\tWorkers: {num_workers}\"\n",
    "          f\"\\t\\tTime: {time_delta : 7.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actual Calculations\n",
    "\n",
    "We loop over different volumes (1GB, 10GB, and 100GB) of double-precision random numbers (float64, 8 bytes each) and estimate $\\pi$ as described above. Note, we call the function with the .compute() method to start the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 GB\n",
      "\tMC pi:  3.14149523200\tErr:  9.742e-05\n",
      "\tWorkers: 2\t\tTime:   0.734s\n",
      "10.0 GB\n",
      "\tMC pi:  3.14155292800\tErr:  3.973e-05\n",
      "\tWorkers: 3\t\tTime:   1.279s\n",
      "100.0 GB\n",
      "\tMC pi:  3.14161598976\tErr:  2.334e-05\n",
      "\tWorkers: 3\t\tTime:   6.973s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (1, 10, 100)):\n",
    "    \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi, time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Cluster to Twice its Size and Re-run the Same Calculations\n",
    "\n",
    "We increase the number of workers times 2 and the re-run the experiments. You could also double the size of the cluster by doubling the number of jobs, cores, or memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling from 3 to 6 workers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.136:45973</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.136:8787/status' target='_blank'>http://10.1.4.136:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>114</li>\n",
       "  <li><b>Memory: </b>342.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.136:45973' processes=3 threads=114, memory=342.00 GB>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_num_workers = 2 * len(cluster.scheduler.workers)\n",
    "\n",
    "print(f\"Scaling from {len(cluster.scheduler.workers)} to {new_num_workers} workers.\")\n",
    "\n",
    "cluster.scale(new_num_workers)\n",
    "\n",
    "# the following commands all get you the same amount of compute resources as above\n",
    "#cluster.scale(12)               # same as code above. default parameter is workers. (original num workers was 6)\n",
    "#cluster.scale(jobs=6)           # can scale by number of jobs\n",
    "#cluster.scale(cores=240)        # can also scale by cores\n",
    "#cluster.scale(memory=600) double check this one\n",
    "\n",
    "sleep(15)\n",
    "client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 GB\n",
      "\tMC pi:  3.14180556800\tErr:  2.129e-04\n",
      "\tWorkers: 3\t\tTime:   0.905s\n",
      "10.0 GB\n",
      "\tMC pi:  3.14153106560\tErr:  6.159e-05\n",
      "\tWorkers: 3\t\tTime:   1.086s\n",
      "100.0 GB\n",
      "\tMC pi:  3.14159702528\tErr:  4.372e-06\n",
      "\tWorkers: 3\t\tTime:   6.675s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (1, 10, 100)):\n",
    "    \n",
    "        \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi,\n",
    "                   time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Scale the Cluster Up and Down (Adaptive Cluster)\n",
    "\n",
    "Using the .adapt() method will dynamically scale up the cluster when necessary but scale it down and save compute resources when not actively computing. Dask will ask the SLURM job scheduler to run more jobs, scaling up the cluster, when workload is high and shut the extra jobs down when workload is smaller.\n",
    "\n",
    "Note that cluster scaling is bound by SCINet HPC user limitations. These limitations on the Ceres HPC are 400 cores, 1512GB memory, and 100 jobs max running simultaneously per user. So for example, if you set your cluster up with 40 cores per job and scale to 20 jobs (40x20=800cores) you will only get 400 cores (10 jobs) running at any time and the remaining requested jobs will not run. Your computations will still run successfully, but they will run on 10 jobs/400 cores instead of the requested 20 jobs/800 cores.\n",
    "\n",
    "_**Watch** how the cluster will scale down to the minimum a few seconds after being made adaptive._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.136:45973</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.136:8787/status' target='_blank'>http://10.1.4.136:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>38</li>\n",
       "  <li><b>Memory: </b>114.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.136:45973' processes=1 threads=38, memory=114.00 GB>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca = cluster.adapt(minimum_jobs=1, maximum_jobs=9);\n",
    "\n",
    "sleep(5)  # Allow for scale-down\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll repeat the calculations with our adaptive cluster and a larger workload. Watch the dash board!\n",
    "\n",
    "WHY DOES THE ADAPTIVE COMPUTE CHOOSE ONLY 1 JOB FOR 10 GB? CAN WE SET A PARAMETER TO MAKE COMPUTE HAPPEN FASTER??? CANT FIND THE API FOR .ADAPT()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 GB\n",
      "\tMC pi:  3.14157190400\tErr:  2.075e-05\n",
      "\tWorkers: 1\t\tTime:   5.114s\n",
      "10.0 GB\n",
      "\tMC pi:  3.14158664320\tErr:  6.010e-06\n",
      "\tWorkers: 1\t\tTime:   5.478s\n",
      "100.0 GB\n",
      "\tMC pi:  3.14156959040\tErr:  2.306e-05\n",
      "\tWorkers: 1\t\tTime:  17.916s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14159782842\tErr:  5.175e-06\n",
      "\tWorkers: 9\t\tTime:  133.827s\n"
     ]
    }
   ],
   "source": [
    "for size in (n * 1e9 for n in (1, 10, 100, 1000)):\n",
    "    \n",
    "    \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size, min(size / 1000, 500e6)).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi, time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))\n",
    "    \n",
    "    sleep(20)  # allow for scale-down time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete listing of software used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list #DO WE REALLY NEED TO PIP AND CONDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda list --explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-tutorial01]",
   "language": "python",
   "name": "conda-env-miniconda3-tutorial01-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
