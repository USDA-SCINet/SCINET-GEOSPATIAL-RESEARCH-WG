{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Computing on Ceres with Python and Dask\n",
    "(adapted from https://github.com/willirath/dask_jobqueue_workshop_materials)\n",
    "\n",
    "## The Goal\n",
    "\n",
    "Interactive data analysis on very large datasets. The tools in this tutorial are most appropriate for analysis of large earth-science-type datasets. \n",
    "\n",
    "For large dataset analysis, you'll want to run parallel (instead of serial) computations to save time. On a high-performance computer (HPC), you could divide your computing into independent segments (batching) and submit multiple batch scripts to run compute jobs simultaneously or you could parallelize your codes using MPI (Message Passing Interface), a traditional method for parallel computing if your code is in C or Fortran. Actually, there is also an \"MPI for Python\" package, but the methods in this tutorial are much *much* simpler. Both the batching and MPI methods of parallelization do not allow for interactive analysis, such as analysis using a Jupyter notebook, which is often desired by the earth science research community. Note that interactive analysis here does *not* mean constant visual presentation of the data with a graphical user interface (GUI) such as in ArcGIS. \n",
    "\n",
    "For earth-science-type data and analysis with Python, one of the simplest ways to run parallel computations in an interactive environment is with the Dask package.\n",
    "\n",
    "\n",
    "## Core Lessons\n",
    "\n",
    "Using Dask to:\n",
    "- set up SLURM clusters\n",
    "- scale clusters\n",
    "- use adaptive clusters\n",
    "- view Dask diagnostics\n",
    "\n",
    "This tutorial will demonstrate how to use Dask to manage compute jobs on a SLURM cluster (including setting up your SLURM compute cluster, scaling the cluster, and how to use an adaptive cluster to save compute resources for others). The tutorial will also explain how to access the Dask diagnostics dashboard to view the cluster working in real time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Tutorial: Parallel Computing on Ceres with Python and Dask\n",
    "\n",
    "In this tutorial we will compute in parallel using Python's Dask package to communicate with the Ceres HPC SLURM job scheduler. \n",
    "\n",
    "SLURM (Simple Linux Utility for Resource Management) is a workload manager for HPC systems. From the [SLURM documentation](https://slurm.schedmd.com/quickstart.html), SLURM is \"an open source... cluster management and job scheduling system for large and small Linux clusters. As a cluster workload manager, SLURM has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## First Set up Your File Space\n",
    "\n",
    "Create a folder in your home directory for the Dask worker error files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/home/kerrie.geil/dask-worker-space-can-be-deleted'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "homedir = os.environ['HOME']\n",
    "daskpath=os.path.join(homedir, \"dask-worker-space-can-be-deleted\")\n",
    "\n",
    "try: \n",
    "    os.mkdir(daskpath) \n",
    "except OSError as error: \n",
    "    print(error) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Set up a SLURM Cluster with Dask\n",
    "\n",
    "The first step is to create a SLURM cluster using the dask.distributed and dask_jobqueue packages. The SLURMCluster function can be interpreted as the settings/parameters for 1 SLURM compute job. Later, we can increase our compute power by \"scaling our cluster\", which means Dask will ask the SLURM scheduler to execute more than one job at a time for any given computation.\n",
    "<br><br>\n",
    "\n",
    "**Here's a key to the dask_jobqueue.SLURMCluster input parameters in the code block below:**\n",
    "\n",
    "**cores** = Number of logical cores per job. This will be divided among the processes/workers. Can't be more than the lowest number of logical cores per node in the queue you choose, see https://scinet.usda.gov/guide/ceres/#partitions-or-queues.\n",
    "   \n",
    "**processes** = Number of processes per job (also known as Dask \"workers\" or \"worker processes\"). The number of cores per worker will be cores/processes. Can use 1 but more than 1 may help keep your computations running if cores/workers fail. For numeric computations (Numpy, Pandas, xarray, etc.), less workers may run significantly faster due to reduction in communication time. If your computations are mostly pure Python, it may be better to run with many workers each associated with only 1 core. [Here is more info than you'll probably ever want to know about Dask workers](https://distributed.dask.org/en/latest/worker.html). \n",
    "\n",
    "**memory** =  Memory per job. This will be divided among the processes/workers. See https://scinet.usda.gov/guide/ceres/#partitions-or-queues for the maximum memory per core you can request on each queue. \n",
    "\n",
    "**queue** = Name of the Ceres queue, a.k.a. partition (e.g. short, medium, long, long60, mem, longmem, mem768, debug, brief-low, scavenger, etc.).\n",
    "\n",
    "**walltime** = Time allowed before the job is timed out.\n",
    "\n",
    "**local_directory** = local spill location if the core memory is exceeded, use /local/scratch a.k.a $TMPDIR \n",
    "\n",
    "**log_directory** = Location to write the stdout and stderr files for each worker process. Simplest choice may be the directory you are running your code from. \n",
    "\n",
    "**python** = The python executable. Add this parameter if you are running in a container to tell SLURM what container and conda env to use. Otherwise, it's not needed.\n",
    "<br><br>\n",
    "\n",
    "You can view additional parameters, methods, and attributes in the Dask documentation for [dask_jobqueue.SLURMCluster](https://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SLURMCluster.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "container='/lustre/project/geospatial_tutorials/wg_2020_ws/data_science_im_rs_vSCINetGeoWS_2020.sif'\n",
    "env='py_geo'\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    cores=40,\n",
    "    processes=1,\n",
    "    memory=\"120GB\", #40 cores x 3GB/core\n",
    "    queue=\"short\",\n",
    "    local_directory='$TMPDIR',\n",
    "    walltime=\"00:10:00\",\n",
    "    log_directory=daskpath,\n",
    "    python=\"singularity -vv exec {} /opt/conda/envs/{}/bin/python\".format(container,env)) #tell the cluster what container and conda env we're using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only set up a cluster, we have not started any compute jobs/workers running yet. We can verify this by issuing the following command in a Ceres terminal. Launch a terminal from the JupyterLab launcher and type:\n",
    "```\n",
    "squeue -u firstname.lastname\n",
    "```\n",
    "\n",
    "To see the job script that will be used to start a job running on the Ceres HPC use the method .job_script() as shown in the code block below.\n",
    "<br><br>\n",
    "\n",
    "**Here's a key to the output of the cluster.job_script() command below:**\n",
    "\n",
    "**-J** = Name of the job. This will appear in the \"Name\" column of the squeue output. \"dask-worker\" is the default value.\n",
    "\n",
    "**-e and -o** = Name/Location of the stdout and stderr files for each job. This comes from the SLURMCLuster \"log_directory\" parameter.\n",
    "\n",
    "**-p** = Name of the Ceres queue/partition. This comes from the SLURMCLuster \"queue\" parameter.\n",
    "\n",
    "**-n** = Number of nodes. \n",
    "\n",
    "**--cpus-per-task** = Number of cores per job (same as -N). This comes from the SLURMCluster \"cores\" parameter.\n",
    "\n",
    "**--mem** = Memory per job. This comes from the SLURMCluster \"memory\" parameter. \n",
    "\n",
    "**-t** = Time allowed before the job is timed out. This comes from the SLURMCluster parameter \"walltime\".\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -e /home/kerrie.geil/dask-worker-space-can-be-deleted/dask-worker-%J.err\n",
      "#SBATCH -o /home/kerrie.geil/dask-worker-space-can-be-deleted/dask-worker-%J.out\n",
      "#SBATCH -p short\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=40\n",
      "#SBATCH --mem=112G\n",
      "#SBATCH -t 00:10:00\n",
      "\n",
      "singularity -vv exec /lustre/project/geospatial_tutorials/wg_2020_ws/data_science_im_rs_vSCINetGeoWS_2020.sif /opt/conda/envs/py_geo/bin/python -m distributed.cli.dask_worker tcp://10.1.4.113:37029 --nthreads 40 --memory-limit 120.00GB --name name --nanny --death-timeout 60 --local-directory $TMPDIR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Next, we must initialize a Dask Client, which opens a line of communication between Dask worker processes and the SLURM job scheduler by pointing to the address of the scheduler (tcp://10.1.8.84:41601).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.113:37029</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.113:8787/status' target='_blank'>http://10.1.4.113:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.113:37029' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: So far we have only set up a cluster and initialized a client. We still have not started any compute jobs running yet, as shown in the Cluster information above. We can also verify that no workers are running yet by issuing the squeue command in a Ceres terminal again as we did previously or we could access the Dask Diagnostics Dashboard for even more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Dask Diagnostics Dashboard\n",
    "\n",
    "We will now take a look at the Dask Dashboard to verify that there a no workers running in our cluster yet. Once we start computing, we will be able to use the Dashboard to see a visual representation of all the workers running.\n",
    "\n",
    "At the very left edge of JupyterLab click the icon that looks like two orange leaves. If the Dask Dashboard extension is working you should see a bunch of orange colored boxes. Each one of these boxes allows you to visualize a different aspect of the compute job.\n",
    "\n",
    "Click on the \"workers\" box to open a separate tab for visualizing the dask workers as they compute. Click over to that tab and right now you should see that there are no workers running yet. When you run a compute job you will see the workers populate the table on your dask workers tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Cluster to Start Computing\n",
    "\n",
    "Now let's start multiple SLURM jobs computing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.113:37029</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.113:8787/status' target='_blank'>http://10.1.4.113:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>80</li>\n",
       "  <li><b>Memory: </b>240.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.113:37029' processes=2 threads=80, memory=240.00 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time, sleep   #time for timing computation length, sleep for pausing while SLURM starts the requested jobs\n",
    "\n",
    "cluster.scale(jobs=3)  # scale to more jobs\n",
    "sleep(15)              # pause while SLURM starts up the jobs\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .scale() method actually starts the jobs running as shown in the Cluster information above. \n",
    "\n",
    "A quick check of squeue will now show your multiple jobs running as well. Or click over to your Dask Workers tab and you'll see you have workers ready to compute.\n",
    "\n",
    "When we set up our original cluster (equivalent of 1 SLURM job) we requested 40 cores spread over 2 workers. When we scaled our cluster to 3 jobs we now have 40x3=120 cores spread over 2x3=6 workers, as shown above. Note: you can also scale your cluster by cores, workers or memory as opposed to jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Estimate of $\\pi$\n",
    "\n",
    "Now we will use the [Monte-Carlo method of estimating $\\pi$](https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods)  to demonstrate how Dask can execute parallel computations with the SLURM Cluster we've built and scaled.\n",
    "\n",
    "We estimate the number $\\pi$ by exploiting that the area of a quarter circle of unit radius is $\\pi/4$ and that hence the probability of any randomly chosen point in a unit square to lie in a unit circle centerd at a corner of the unit square is $\\pi/4$ as well. \n",
    "\n",
    "So for N randomly chosen pairs $(x, y)$ with $x\\in[0, 1)$ and $y\\in[0, 1)$, we count the number $N_{circ}$ of pairs that also satisfy $(x^2 + y^2) < 1$ and estimate $\\pi \\approx 4 \\cdot N_{circ} / N$.\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/84/Pi_30K.gif\" \n",
    "     width=\"50%\" \n",
    "     align=top\n",
    "     alt=\"PI monte-carlo estimate\">](https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Let's define a function to compute $\\pi$ and another function to print out some info during the compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "def calc_pi_mc(size_in_bytes, chunksize_in_bytes=200e6):  # IS THERE A REASON TO CHOOSE THIS CHUNKSIZE\n",
    "    \"\"\"Calculate PI using a Monte Carlo estimate.\"\"\"\n",
    "    \n",
    "    size = int(size_in_bytes / 8)      # size= # of random numbers to generate (x & y vals), divide 8 bcz numpy float64 numbers generated by random.uniform use up 8 bytes\n",
    "    chunksize = int(chunksize_in_bytes / 8)\n",
    "    \n",
    "    xy = da.random.uniform(0, 1,                          # this generates a set of x and y value pairs on the interval [0,1) of type float64\n",
    "                           size=(size / 2, 2),            # divide 2 because we are generating an equal number of x and y values (to get our points)\n",
    "                           chunks=(chunksize / 2, 2))     # WHY WOULD WE DIVIDE THE CHUNKSIZE BY 2 HERE????\n",
    "    \n",
    "    in_circle = ((xy ** 2).sum(axis=-1) < 1)     # a boolean array, True for points that fall inside the unit circle (x**2 + y**2 < 1)\n",
    "    pi = 4 * in_circle.mean()                    # mean= sum the number of True elements, divide by the total number of elements in the array\n",
    "\n",
    "    return pi\n",
    "\n",
    "def print_pi_stats(size, pi, time_delta, num_workers):  \n",
    "    \"\"\"Print pi, calculate offset from true value, and print some stats.\"\"\"\n",
    "    print(f\"{size / 1e9} GB\\n\"\n",
    "          f\"\\tMC pi: {pi : 13.11f}\"\n",
    "          f\"\\tErr: {abs(pi - np.pi) : 10.3e}\\n\"\n",
    "          f\"\\tWorkers: {num_workers}\"\n",
    "          f\"\\t\\tTime: {time_delta : 7.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actual Calculations\n",
    "\n",
    "We loop over different volumes (1GB, 10GB, and 100GB) of double-precision random numbers (float64, 8 bytes each) and estimate $\\pi$ as described above. Note, we call the function with the .compute() method to start the computations. To see the dask workers computing, execute the code block below and then quickly click over to your dask workers tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 GB\n",
      "\tMC pi:  3.14150342400\tErr:  8.923e-05\n",
      "\tWorkers: 3\t\tTime:   0.951s\n",
      "10.0 GB\n",
      "\tMC pi:  3.14162090240\tErr:  2.825e-05\n",
      "\tWorkers: 3\t\tTime:   1.231s\n",
      "100.0 GB\n",
      "\tMC pi:  3.14158821696\tErr:  4.437e-06\n",
      "\tWorkers: 3\t\tTime:   7.127s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (1, 10, 100)):\n",
    "    \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi, time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Cluster to Twice its Size and Re-run the Same Calculations\n",
    "\n",
    "We increase the number of workers times 2 and the re-run the experiments. You could also double the size of the cluster by doubling the number of jobs, cores, or memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling from 3 to 6 workers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.113:37029</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.113:8787/status' target='_blank'>http://10.1.4.113:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>160</li>\n",
       "  <li><b>Memory: </b>480.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.113:37029' processes=4 threads=160, memory=480.00 GB>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_num_workers = 2 * len(cluster.scheduler.workers)\n",
    "\n",
    "print(f\"Scaling from {len(cluster.scheduler.workers)} to {new_num_workers} workers.\")\n",
    "\n",
    "cluster.scale(new_num_workers)\n",
    "\n",
    "# the following commands all get you the same amount of compute resources as above\n",
    "#cluster.scale(12)               # same as code above. default parameter is workers. (original num workers was 6)\n",
    "#cluster.scale(jobs=6)           # can scale by number of jobs\n",
    "#cluster.scale(cores=240)        # can also scale by cores\n",
    "#cluster.scale(memory=600) double check this one\n",
    "\n",
    "sleep(15)\n",
    "client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 GB\n",
      "\tMC pi:  3.14160492800\tErr:  1.227e-05\n",
      "\tWorkers: 6\t\tTime:   0.909s\n",
      "10.0 GB\n",
      "\tMC pi:  3.14157890560\tErr:  1.375e-05\n",
      "\tWorkers: 6\t\tTime:   1.066s\n",
      "100.0 GB\n",
      "\tMC pi:  3.14157606400\tErr:  1.659e-05\n",
      "\tWorkers: 6\t\tTime:   4.722s\n"
     ]
    }
   ],
   "source": [
    "for size in (1e9 * n for n in (1, 10, 100)):\n",
    "    \n",
    "        \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi,\n",
    "                   time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Scale the Cluster Up and Down (Adaptive Cluster)\n",
    "\n",
    "Using the .adapt() method will dynamically scale up the cluster when necessary but scale it down and save compute resources when not actively computing. Dask will ask the SLURM job scheduler to run more jobs, scaling up the cluster, when workload is high and shut the extra jobs down when workload is smaller.\n",
    "\n",
    "Note that cluster scaling is bound by SCINet HPC user limitations. These limitations on the Ceres HPC are 400 cores, 1512GB memory, and 100 jobs max running simultaneously per user. So for example, if you set your cluster up with 40 cores per job and scale to 20 jobs (40x20=800cores) you will only get 400 cores (10 jobs) running at any time and the remaining requested jobs will not run. Your computations will still run successfully, but they will run on 10 jobs/400 cores instead of the requested 20 jobs/800 cores.\n",
    "\n",
    "_**Watch** how the cluster will scale down to the minimum a few seconds after being made adaptive._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.113:37029</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.113:8787/status' target='_blank'>http://10.1.4.113:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>40</li>\n",
       "  <li><b>Memory: </b>120.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.113:37029' processes=1 threads=40, memory=120.00 GB>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca = cluster.adapt(minimum_jobs=1, maximum_jobs=9);\n",
    "\n",
    "sleep(5)  # Allow for scale-down\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll repeat the calculations with our adaptive cluster and a larger workload. Watch the dash board!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 GB\n",
      "\tMC pi:  3.14126041600\tErr:  3.322e-04\n",
      "\tWorkers: 1\t\tTime:   6.763s\n",
      "10.0 GB\n",
      "\tMC pi:  3.14149736320\tErr:  9.529e-05\n",
      "\tWorkers: 5\t\tTime:   2.534s\n",
      "100.0 GB\n",
      "\tMC pi:  3.14157973440\tErr:  1.292e-05\n",
      "\tWorkers: 9\t\tTime:   3.851s\n",
      "1000.0 GB\n",
      "\tMC pi:  3.14157613658\tErr:  1.652e-05\n",
      "\tWorkers: 9\t\tTime:  24.136s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.4.113:37029</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.4.113:8787/status' target='_blank'>http://10.1.4.113:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>40</li>\n",
       "  <li><b>Memory: </b>120.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.4.113:37029' processes=1 threads=40, memory=120.00 GB>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for size in (n * 1e9 for n in (1, 10, 100, 1000)):\n",
    "    \n",
    "    start = time()\n",
    "    pi = calc_pi_mc(size, min(size / 1000, 500e6)).compute()\n",
    "    elaps = time() - start\n",
    "\n",
    "    print_pi_stats(size, pi, time_delta=elaps,\n",
    "                   num_workers=len(cluster.scheduler.workers))\n",
    "    \n",
    "sleep(5)  # allow for scale-down time\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the use cases for the adaptive cluster feature? Personally, I will be using the adaptive cluster when I have a code that contains a mix of lighter and heavier computations so I can use the minimum number of cores necessary for the lighter parts of the code and then have my cluster automagically scale up to handle heavier parts of the code without me having to think about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete listing of software used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda list --explicit"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py_geo]",
   "language": "python",
   "name": "conda-env-py_geo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
